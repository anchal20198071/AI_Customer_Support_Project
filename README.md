# ğŸ§  AI Customer Support Assistant (RAG + LangGraph)

An end-to-end AI-powered customer support assistant built using **React**, **FastAPI**, **LangChain**, **LangGraph**, and **local LLMs (Ollama)**.

The system supports **document upload**, **retrieval-augmented generation (RAG)**, **clarification-aware reasoning**, and **multi-document querying**.

---

## âœ¨ Features

- ğŸ“„ **PDF Upload from UI** (Knowledge Ingestion)
- ğŸ” **Retrieval-Augmented Generation (RAG)** using Chroma DB
- ğŸ§  **LangGraph-based control flow**
- â“ Clarifies ambiguous user queries
- ğŸš« Prevents hallucinations
- ğŸ¤– Local LLM support via **Ollama (Mistral)**
- ğŸ’¬ ChatGPT-style React UI
- ğŸ“š Multi-document support
- âš¡ FastAPI backend with OpenAPI docs

---

## ğŸ—ï¸ Architecture Overview

### ğŸ”· High-Level Flow

User (React UI)  
&nbsp;&nbsp;â†“  
1. Upload PDF / Ask Question via FastAPI Backend  
&nbsp;&nbsp;â”œâ”€â”€ **/upload** â†’ PDF ingestion & vectorization  
&nbsp;&nbsp;â””â”€â”€ **/chat** â†’ LangGraph execution  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â†“  
LangGraph Orchestration  
&nbsp;&nbsp;â”œâ”€â”€ Intent / Clarity Check  
&nbsp;&nbsp;â”œâ”€â”€ Clarifying Question (if needed)  
&nbsp;&nbsp;â”œâ”€â”€ RAG Node (Vector Retrieval via ChromaDB)  
&nbsp;&nbsp;â””â”€â”€ LLM Reasoning (Mistral via Ollama)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â†“  
Final response returned to the UI

---

### ğŸ”· LangGraph Control Flow

User Question  
&nbsp;&nbsp;â†“  
Router Node (Is the question clear?)  
&nbsp;&nbsp;â”œâ”€â”€ âŒ No â†’ Ask Clarifying Question  
&nbsp;&nbsp;â””â”€â”€ âœ… Yes  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â†“  
Retrieve Relevant Documents (ChromaDB)  
&nbsp;&nbsp;â†“  
LLM Reasoning (Mistral via Ollama)  
&nbsp;&nbsp;â†“  
Final Answer

---

### âœ… This Ensures

- Ambiguous questions donâ€™t trigger hallucinations
- Answers are grounded in uploaded documents
- Multi-document contexts are handled correctly

---

## ğŸ§  Retrieval-Augmented Generation (RAG)

- PDFs are split into chunks
- Chunks are embedded using sentence-transformers
- Stored in **Chroma Vector DB**
- Relevant chunks retrieved per query
- LLM answers only using retrieved context

If relevant information is missing, the assistant responds with:

> â€œI donâ€™t have enough information to answer that.â€

---

## ğŸ–¥ï¸ Tech Stack

### Frontend

- React (Vite)
- Fetch API
- Chat-style UI (Flexbox)

### Backend

- FastAPI(Python)
- LangChain
- LangGraph
- ChromaDB
- Ollama (Mistral LLM)
- HuggingFace Embeddings

---

## ğŸ“¸ Screenshots

![Chat UI](screenshots/chat-ui.png)
![PDF Upload](screenshots/upload.png)
![Clarification](screenshots/clarification.png)


<img width="1040" height="888" alt="image" src="https://github.com/user-attachments/assets/af2f85c2-721e-4dfa-9b47-ceb5b5155c16" />

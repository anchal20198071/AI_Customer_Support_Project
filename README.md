# ğŸ§  AI Customer Support Assistant (RAG + LangGraph)

An AI-powered customer support assistant that answers user questions from uploaded documents using Retrieval-Augmented Generation (RAG). The system intelligently handles ambiguous queries, asks clarifying questions, and produces document-grounded answers using a LangGraph-based reasoning flow.

An end-to-end AI-powered customer support assistant built using **React**, **FastAPI**, **LangChain**, **LangGraph**, and **local LLMs (Ollama)**.

The system supports **document upload**, **retrieval-augmented generation (RAG)**, **clarification-aware reasoning**, and **multi-document querying**.

---

## âœ¨ Features

- ğŸ“„ **PDF Upload from UI** (Knowledge Ingestion)
- ğŸ” **Retrieval-Augmented Generation (RAG)** using Chroma DB
- ğŸ§  **LangGraph-based control flow**
- â“ Clarifies ambiguous user queries
- ğŸš« Prevents hallucinations
- ğŸ¤– Local LLM support via **Ollama (Mistral)**
- ğŸ’¬ ChatGPT-style React UI
- ğŸ“š Multi-document support
- âš¡ FastAPI backend with OpenAPI docs

---

## ğŸ—ï¸ Architecture Overview

### ğŸ”· High-Level Flow

User (React UI)  
&nbsp;&nbsp;â†“  
1. Upload PDF / Ask Question via FastAPI Backend  
&nbsp;&nbsp;â”œâ”€â”€ **/upload** â†’ PDF ingestion & vectorization  
&nbsp;&nbsp;â””â”€â”€ **/chat** â†’ LangGraph execution  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â†“  
LangGraph Orchestration  
&nbsp;&nbsp;â”œâ”€â”€ Intent / Clarity Check  
&nbsp;&nbsp;â”œâ”€â”€ Clarifying Question (if needed)  
&nbsp;&nbsp;â”œâ”€â”€ RAG Node (Vector Retrieval via ChromaDB)  
&nbsp;&nbsp;â””â”€â”€ LLM Reasoning (Mistral via Ollama)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â†“  
Final response returned to the UI

---

### ğŸ”· LangGraph Control Flow

User Question  
&nbsp;&nbsp;â†“  
Router Node (Is the question clear?)  
&nbsp;&nbsp;â”œâ”€â”€ âŒ No â†’ Ask Clarifying Question  
&nbsp;&nbsp;â””â”€â”€ âœ… Yes  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â†“  
Retrieve Relevant Documents (ChromaDB)  
&nbsp;&nbsp;â†“  
LLM Reasoning (Mistral via Ollama)  
&nbsp;&nbsp;â†“  
Final Answer

---

### âœ… This Ensures

- Ambiguous questions donâ€™t trigger hallucinations
- Answers are grounded in uploaded documents
- Multi-document contexts are handled correctly

---

## ğŸ§  Retrieval-Augmented Generation (RAG)

- PDFs are split into chunks
- Chunks are embedded using sentence-transformers
- Stored in **Chroma Vector DB**
- Relevant chunks retrieved per query
- LLM answers only using retrieved context

If relevant information is missing, the assistant responds with:

> â€œI donâ€™t have enough information to answer that.â€

---

## ğŸ–¥ï¸ Tech Stack

### Frontend

- React (Vite)
- Fetch API
- Chat-style UI (Flexbox)

### Backend

- FastAPI(Python)
- LangChain
- LangGraph
- ChromaDB
- Ollama (Mistral LLM)
- HuggingFace Embeddings

---

## ğŸ“¸ Screenshots
- Chat UI Screenshot
<img width="1155" height="881" alt="image" src="https://github.com/user-attachments/assets/f9628cb4-174e-4df9-aaa7-7c2b9107eb20" />

- PDF Upload Screenshot
<img width="1154" height="884" alt="image" src="https://github.com/user-attachments/assets/3c2048ed-3525-4efe-80d0-12fab334d403" />

- RAG
<img width="911" height="816" alt="image" src="https://github.com/user-attachments/assets/9c6fdb4b-cda1-41eb-acae-3a8eaa21ee7f" />
<img width="919" height="764" alt="image" src="https://github.com/user-attachments/assets/319b6294-2232-4355-80e9-8f004fb97a3b" />

- Out-of-Scope (Hallucination Test)
<img width="904" height="777" alt="image" src="https://github.com/user-attachments/assets/d91628f3-dd1c-4c55-8b09-44bcde22f083" />
